{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastcore.basics import Path, AttrDict\n",
    "import utils\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_paths = [\n",
    "    os.path.abspath(os.path.join('../..')),\n",
    "    os.path.abspath(os.path.join('.')),\n",
    "]\n",
    "for module_path in module_paths:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "\n",
    "# This is used to import the evaluation script, not needed for training\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "import evaluation\n",
    "import warnings \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    # challenge_data_dir = Path('../../dataset/data_subset/'),\n",
    "    challenge_data_dir = Path('../../dataset/phase_1_v3/'),\n",
    "    valid_ratio = .5,\n",
    "    lag_steps = 0,\n",
    "    tolerance= 6, # Default evaluation tolerance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of feature columns\n",
    "delta_column = \"Delta_SemimajorAxis\"\n",
    "inclination_diff = \"Inclination Diff\"\n",
    "\n",
    "feature_cols = [\n",
    "    \"Eccentricity\",\n",
    "    \"Semimajor Axis (m)\",\n",
    "    \"Inclination (deg)\",\n",
    "    \"RAAN (deg)\",\n",
    "    \"Argument of Periapsis (deg)\",\n",
    "    \"True Anomaly (deg)\",\n",
    "    # \"Latitude (deg)\",\n",
    "    \"Longitude (deg)\",\n",
    "    \"Altitude (m)\",\n",
    "    # \"X (m)\",\n",
    "    # \"Y (m)\",\n",
    "    # \"Z (m)\",\n",
    "    # \"Vx (m/s)\",\n",
    "    # \"Vy (m/s)\",\n",
    "    # \"Vz (m/s)\",\n",
    "    # delta_column,\n",
    "    # inclination_diff,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Semimajor Axis (m)</th>\n",
       "      <th>Inclination (deg)</th>\n",
       "      <th>RAAN (deg)</th>\n",
       "      <th>Argument of Periapsis (deg)</th>\n",
       "      <th>True Anomaly (deg)</th>\n",
       "      <th>Latitude (deg)</th>\n",
       "      <th>Longitude (deg)</th>\n",
       "      <th>Altitude (m)</th>\n",
       "      <th>...</th>\n",
       "      <th>Vx (m/s)</th>\n",
       "      <th>Vy (m/s)</th>\n",
       "      <th>Vz (m/s)</th>\n",
       "      <th>ObjectID</th>\n",
       "      <th>TimeIndex</th>\n",
       "      <th>Delta_SemimajorAxis</th>\n",
       "      <th>Inclination Diff</th>\n",
       "      <th>Inclination Direction</th>\n",
       "      <th>EW</th>\n",
       "      <th>NS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-01 00:00:00.000000Z</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>4.216537e+07</td>\n",
       "      <td>0.139822</td>\n",
       "      <td>94.427336</td>\n",
       "      <td>52.733092</td>\n",
       "      <td>277.810980</td>\n",
       "      <td>-0.014460</td>\n",
       "      <td>85.119506</td>\n",
       "      <td>3.578607e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>-2786.228658</td>\n",
       "      <td>1300.258746</td>\n",
       "      <td>6.534142</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>SS-HK</td>\n",
       "      <td>SS-HK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-01 02:00:00.000000Z</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>4.216458e+07</td>\n",
       "      <td>0.140008</td>\n",
       "      <td>94.407240</td>\n",
       "      <td>59.443909</td>\n",
       "      <td>301.205730</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>85.122842</td>\n",
       "      <td>3.578177e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>-3062.958599</td>\n",
       "      <td>-271.603808</td>\n",
       "      <td>7.513546</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1</td>\n",
       "      <td>SS-HK</td>\n",
       "      <td>SS-HK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-01 04:00:00.000000Z</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>4.216450e+07</td>\n",
       "      <td>0.140139</td>\n",
       "      <td>94.420500</td>\n",
       "      <td>64.355257</td>\n",
       "      <td>326.372111</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>85.131695</td>\n",
       "      <td>3.577825e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>-2514.294041</td>\n",
       "      <td>-1770.701547</td>\n",
       "      <td>6.465207</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-870.131537</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>1</td>\n",
       "      <td>SS-HK</td>\n",
       "      <td>SS-HK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-01 06:00:00.000000Z</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>4.216524e+07</td>\n",
       "      <td>0.140147</td>\n",
       "      <td>94.419710</td>\n",
       "      <td>67.664873</td>\n",
       "      <td>353.158326</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>85.144601</td>\n",
       "      <td>3.577642e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>-1287.802531</td>\n",
       "      <td>-2792.783114</td>\n",
       "      <td>3.667072</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>653.280794</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1</td>\n",
       "      <td>SS-HK</td>\n",
       "      <td>SS-HK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-01 08:00:00.000000Z</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>4.216620e+07</td>\n",
       "      <td>0.140112</td>\n",
       "      <td>94.348243</td>\n",
       "      <td>70.366450</td>\n",
       "      <td>20.624466</td>\n",
       "      <td>0.015595</td>\n",
       "      <td>85.158717</td>\n",
       "      <td>3.577713e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>285.868898</td>\n",
       "      <td>-3062.068198</td>\n",
       "      <td>-0.129326</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1707.623420</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-1</td>\n",
       "      <td>SS-HK</td>\n",
       "      <td>SS-HK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Timestamp  Eccentricity  Semimajor Axis (m)  \\\n",
       "0  2022-09-01 00:00:00.000000Z      0.000202        4.216537e+07   \n",
       "1  2022-09-01 02:00:00.000000Z      0.000214        4.216458e+07   \n",
       "2  2022-09-01 04:00:00.000000Z      0.000231        4.216450e+07   \n",
       "3  2022-09-01 06:00:00.000000Z      0.000255        4.216524e+07   \n",
       "4  2022-09-01 08:00:00.000000Z      0.000277        4.216620e+07   \n",
       "\n",
       "   Inclination (deg)  RAAN (deg)  Argument of Periapsis (deg)  \\\n",
       "0           0.139822   94.427336                    52.733092   \n",
       "1           0.140008   94.407240                    59.443909   \n",
       "2           0.140139   94.420500                    64.355257   \n",
       "3           0.140147   94.419710                    67.664873   \n",
       "4           0.140112   94.348243                    70.366450   \n",
       "\n",
       "   True Anomaly (deg)  Latitude (deg)  Longitude (deg)  Altitude (m)  ...  \\\n",
       "0          277.810980       -0.014460        85.119506  3.578607e+07  ...   \n",
       "1          301.205730       -0.007812        85.122842  3.578177e+07  ...   \n",
       "2          326.372111        0.001030        85.131695  3.577825e+07  ...   \n",
       "3          353.158326        0.009633        85.144601  3.577642e+07  ...   \n",
       "4           20.624466        0.015595        85.158717  3.577713e+07  ...   \n",
       "\n",
       "      Vx (m/s)     Vy (m/s)  Vz (m/s)  ObjectID  TimeIndex  \\\n",
       "0 -2786.228658  1300.258746  6.534142         1          0   \n",
       "1 -3062.958599  -271.603808  7.513546         1          1   \n",
       "2 -2514.294041 -1770.701547  6.465207         1          2   \n",
       "3 -1287.802531 -2792.783114  3.667072         1          3   \n",
       "4   285.868898 -3062.068198 -0.129326         1          4   \n",
       "\n",
       "   Delta_SemimajorAxis  Inclination Diff  Inclination Direction     EW     NS  \n",
       "0             0.000000          0.000000                     -1  SS-HK  SS-HK  \n",
       "1             0.000000          0.000187                      1  SS-HK  SS-HK  \n",
       "2          -870.131537          0.000131                      1  SS-HK  SS-HK  \n",
       "3           653.280794          0.000008                      1  SS-HK  SS-HK  \n",
       "4          1707.623420         -0.000035                     -1  SS-HK  SS-HK  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory paths\n",
    "train_data_dir = config.challenge_data_dir / \"train\"\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truth = pd.read_csv(config.challenge_data_dir / 'train_labels.csv')\n",
    "\n",
    "# Apply the function to the ground truth data\n",
    "data, updated_feature_cols = utils.tabularize_data(train_data_dir, feature_cols, \n",
    "                                          ground_truth, lag_steps=config.lag_steps)\n",
    "\n",
    "# For each ObjectID, show the first rows of the columns TimeIndex, ObjectID, EW, and NS\n",
    "# data[['ObjectID', 'TimeIndex' , 'EW', 'NS']].groupby('ObjectID').head(2).head(10)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the training set: 950\n",
      "Number of objects in the validation set: 950\n"
     ]
    }
   ],
   "source": [
    "# Create a validation set without mixing the ObjectIDs\n",
    "object_ids = data['ObjectID'].unique()\n",
    "train_ids, valid_ids = train_test_split(object_ids, \n",
    "                                        test_size=config.valid_ratio)\n",
    "\n",
    "train_data = data[data['ObjectID'].isin(train_ids)].copy()\n",
    "valid_data = data[data['ObjectID'].isin(valid_ids)].copy()\n",
    "\n",
    "ground_truth_train = ground_truth[ground_truth['ObjectID'].isin(train_ids)].copy()\n",
    "ground_truth_valid = ground_truth[ground_truth['ObjectID'].isin(valid_ids)].copy()\n",
    "\n",
    "# Count the number of objects in the training and validation sets\n",
    "print('Number of objects in the training set:', len(train_data['ObjectID'].unique()))\n",
    "print('Number of objects in the validation set:', len(valid_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make sure that there every label, both in the direction EW and NS,\n",
    "is present both in the training and validation partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of EW in test data: set()\n",
      "Missing values of NS in test data: set()\n",
      "Values of EW not present in NS: {'AD-NK'}\n"
     ]
    }
   ],
   "source": [
    "# Get the unique values of EW and NS in train and test data\n",
    "train_EW = set(train_data['EW'].unique())\n",
    "train_NS = set(train_data['NS'].unique())\n",
    "valid_EW = set(valid_data['EW'].unique())\n",
    "valid_NS = set(valid_data['NS'].unique())\n",
    "\n",
    "# Get the values of EW and NS that are in test data but not in train data\n",
    "missing_EW = valid_EW.difference(train_EW)\n",
    "missing_NS = valid_NS.difference(train_NS)\n",
    "\n",
    "# Check if all the values in EW are also present in NS\n",
    "if not set(train_data['EW'].unique()).issubset(set(train_data['NS'].unique())):\n",
    "    # Get the values of EW that are not present in NS\n",
    "    missing_EW_NS = set(train_data['EW'].unique()).difference(\n",
    "        set(train_data['NS'].unique())\n",
    "    )\n",
    "else:\n",
    "    missing_EW_NS = None\n",
    "\n",
    "# Print the missing values of EW and NS\n",
    "print(\"Missing values of EW in test data:\", missing_EW)\n",
    "print(\"Missing values of NS in test data:\", missing_NS)\n",
    "print(\"Values of EW not present in NS:\", missing_EW_NS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns encoded\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 58.6 GiB for an array with shape (3808, 2064504) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define the Random Forest model for EW\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# model_EW = RandomForestClassifier(n_estimators=200, random_state=42)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Define the Random Forest model for NS\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model_NS \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m model_NS\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mupdated_feature_cols\u001b[49m\u001b[43m]\u001b[49m, train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNS_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     19\u001b[0m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(model_NS, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_model/model_NS_v3.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\frame.py:4105\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   4103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 4105\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[0;32m   4108\u001b[0m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[0;32m   4109\u001b[0m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[0;32m   4110\u001b[0m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[0;32m   4112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n\u001b[0;32m   4113\u001b[0m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\generic.py:4150\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4139\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4148\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4150\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4151\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\generic.py:4130\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4125\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4126\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4127\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4128\u001b[0m     )\n\u001b[1;32m-> 4130\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4132\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4136\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4137\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 58.6 GiB for an array with shape (3808, 2064504) and data type float64"
     ]
    }
   ],
   "source": [
    "# Convert categorical data to numerical data\n",
    "le_EW = LabelEncoder()\n",
    "le_NS = LabelEncoder()\n",
    "\n",
    "# Encode the 'EW' and 'NS' columns\n",
    "train_data['EW_encoded'] = le_EW.fit_transform(train_data['EW'])\n",
    "train_data['NS_encoded'] = le_NS.fit_transform(train_data['NS'])\n",
    "valid_data['Predicted_EW'] = 'ES-ES'\n",
    "\n",
    "print(\"Columns encoded\")\n",
    "\n",
    "# Define the Random Forest model for EW\n",
    "# model_EW = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# Define the Random Forest model for NS\n",
    "model_NS = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "model_NS.fit(train_data[updated_feature_cols], train_data['NS_encoded'])\n",
    "\n",
    "Path('trained_model').mkdir(exist_ok=True)\n",
    "pickle.dump(model_NS, open('trained_model/model_NS_v3.pkl', 'wb'))\n",
    "pickle.dump(le_NS, open('trained_model/le_NS_v3.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_kbest_score(min, max, score_list, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.arange(min, max+1)\n",
    "    y = score_list\n",
    "\n",
    "    ax.bar(x,y,width = 0.2)\n",
    "    ax.set_xlabel('Number of features selected')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_ylim(0,1.2)\n",
    "    ax.set_xticks(np.arange(min,max+1))\n",
    "    ax.set_xticklabels(np.arange(min, max+1), fontsize=12)\n",
    "\n",
    "    for i, v in enumerate(y):\n",
    "        plt.text(x=i+min, y=v+0.05, s=str(v), ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns encoded\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 58.7 GiB for an array with shape (3816, 2064492) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_min,k_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     42\u001b[0m     selector \u001b[38;5;241m=\u001b[39m SelectKBest(mutual_info_classif, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m---> 43\u001b[0m     selector\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mupdated_feature_cols\u001b[49m\u001b[43m]\u001b[49m, train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNS_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     44\u001b[0m     selector_mask \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mget_support()\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(train_data[updated_feature_cols]\u001b[38;5;241m.\u001b[39mcolumns[selector_mask])\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\frame.py:4105\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   4103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 4105\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[0;32m   4108\u001b[0m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[0;32m   4109\u001b[0m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[0;32m   4110\u001b[0m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[0;32m   4112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n\u001b[0;32m   4113\u001b[0m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\generic.py:4150\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4139\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4148\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4150\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4151\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\generic.py:4130\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4125\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4126\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4127\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4128\u001b[0m     )\n\u001b[1;32m-> 4130\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4132\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4136\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4137\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joe\\anaconda3\\envs\\baseml\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 58.7 GiB for an array with shape (3816, 2064492) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import ns_id_Validator\n",
    "import ns_ik_Validator\n",
    "import ew_Validator\n",
    "import ss_Validator\n",
    "\n",
    "# TRAINED_MODEL_DIR = Path('./trained_model/')\n",
    "\n",
    "# # Load the trained models (don't use the utils module, use pickle)\n",
    "# model_EW = pickle.load(open(TRAINED_MODEL_DIR / 'model_EW_v3.pkl', 'rb'))\n",
    "# le_EW = pickle.load(open(TRAINED_MODEL_DIR / 'le_EW_v3.pkl', 'rb'))\n",
    "# model_NS = pickle.load(open(TRAINED_MODEL_DIR / 'model_NS_v3.pkl', 'rb'))\n",
    "# le_NS = pickle.load(open(TRAINED_MODEL_DIR / 'le_NS_v3.pkl', 'rb'))\n",
    "\n",
    "# Convert categorical data to numerical data\n",
    "le_EW = LabelEncoder()\n",
    "le_NS = LabelEncoder()\n",
    "\n",
    "# Encode the 'EW' and 'NS' columns\n",
    "train_data['EW_encoded'] = le_EW.fit_transform(train_data['EW'])\n",
    "train_data['NS_encoded'] = le_NS.fit_transform(train_data['NS'])\n",
    "valid_data['Predicted_EW'] = 'ES-ES'\n",
    "\n",
    "print(\"Columns encoded\")\n",
    "\n",
    "# Define the Random Forest model for EW\n",
    "# model_EW = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# Define the Random Forest model for NS\n",
    "model_NS = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "f2_score_list = []\n",
    "recall_list = []\n",
    "precision_list = []\n",
    "ss_validator = ss_Validator.SS_Validator()\n",
    "ns_ik_validator = ns_ik_Validator.NS_IK_Validator()\n",
    "ns_id_validator = ns_id_Validator.NS_ID_Validator()\n",
    "\n",
    "k_min = 3\n",
    "k_max = 8\n",
    "for k in range(k_min,k_max+1):\n",
    "    selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    selector.fit(train_data[updated_feature_cols], train_data['NS_encoded'])\n",
    "    selector_mask = selector.get_support()\n",
    "    print(train_data[updated_feature_cols].columns[selector_mask])\n",
    "\n",
    "    sel_X_train = selector.transform(train_data[updated_feature_cols])\n",
    "    sel_X_test = selector.transform(valid_data[updated_feature_cols])\n",
    "\n",
    "\n",
    "    # Fit the model to the training data for NS\n",
    "    model_NS.fit(sel_X_train, train_data['NS_encoded'])\n",
    "\n",
    "    print(\"NS Classifier is Fit for k=\" +str(k))\n",
    "\n",
    "    # Make predictions on the training data for NS\n",
    "    valid_data['Predicted_NS'] = le_NS.inverse_transform(\n",
    "        model_NS.predict(sel_X_test)\n",
    "    )\n",
    "\n",
    "    test_results = utils.convert_classifier_output(valid_data)\n",
    "    validated_results = ss_validator.apply_validator(test_results, valid_data)\n",
    "\n",
    "    validated_results = ns_ik_validator.apply_validator(validated_results, valid_data)\n",
    "\n",
    "    # validated_results = ns_id_validator.apply_validator(validated_results, valid_data)\n",
    "\n",
    "    evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, validated_results, \n",
    "                                                tolerance=config.tolerance)\n",
    "    precision, recall, f2, rmse = evaluator.score_NS(debug=True)\n",
    "    f2_score_list.append(round(f2, 3))\n",
    "    print(\"F2 Score: \" + str(round(f2, 3)))\n",
    "    recall_list.append(round(recall, 3))\n",
    "    print(\"Recall: \" + str(round(recall, 3)))\n",
    "    precision_list.append(round(precision,3))\n",
    "    print(\"Precision: \" + str(round(precision, 3)))\n",
    "    print(\"RMSE: \" + str(round(rmse, 3)))\n",
    "\n",
    "\n",
    "plot_kbest_score(k_min, k_max, f2_score_list, \"F2 Score\")\n",
    "plot_kbest_score(k_min, k_max, recall_list, \"Recall\")\n",
    "plot_kbest_score(k_min, k_max, precision_list, \"Precision\")\n",
    "    \n",
    "Path('trained_model').mkdir(exist_ok=True)\n",
    "pickle.dump(model_NS, open('trained_model/model_NS_v3.pkl', 'wb'))\n",
    "pickle.dump(le_NS, open('trained_model/le_NS_v3.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = utils.convert_classifier_output(valid_data)\n",
    "validated_results = ss_validator.apply_validator(test_results, valid_data)\n",
    "\n",
    "print(\"Before EW Validator:\")\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, validated_results, \n",
    "                                            tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score_EW(debug=True)\n",
    "f2_score_list.append(round(f2, 3))\n",
    "print(\"F2 Score: \" + str(round(f2, 3)))\n",
    "recall_list.append(round(recall, 3))\n",
    "print(\"Recall: \" + str(round(recall, 3)))\n",
    "precision_list.append(round(precision,3))\n",
    "print(\"Precision: \" + str(round(precision, 3)))\n",
    "print(\"RMSE: \" + str(round(rmse, 3)))\n",
    "\n",
    "\n",
    "print(\"After EW Validator:\")\n",
    "validated_results = ew_validator.apply_validator(validated_results, valid_data)\n",
    "\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, validated_results, \n",
    "                                            tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score_EW(debug=True)\n",
    "f2_score_list.append(round(f2, 3))\n",
    "print(\"F2 Score: \" + str(round(f2, 3)))\n",
    "recall_list.append(round(recall, 3))\n",
    "print(\"Recall: \" + str(round(recall, 3)))\n",
    "precision_list.append(round(precision,3))\n",
    "print(\"Precision: \" + str(round(precision, 3)))\n",
    "print(\"RMSE: \" + str(round(rmse, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(mutual_info_classif, k=6) # set the best k\n",
    "selector.fit(train_data[updated_feature_cols], train_data['EW_encoded'])\n",
    "selected_feature_mask = selector.get_support()\n",
    "\n",
    "selected_features = train_data[updated_feature_cols].columns[selected_feature_mask]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ss_validator(results, data):\n",
    "        mask = (results['Node']=='SS')&(results['TimeIndex']!=0)\n",
    "        stripped_results = results[~mask]\n",
    "        merged_results = stripped_results.copy()\n",
    "        merged_results = merged_results.sort_values(by=['ObjectID', 'TimeIndex']).reset_index(drop=True)\n",
    "        return merged_results\n",
    "\n",
    "DELTA_COLUMN = \"Delta_SemimajorAxis\"\n",
    "MANEUVER_THRESHOLD = 3000\n",
    "def apply_ew_maneuver_validator(results, data):\n",
    "        validate_results = results[(results['Node']!='SS') & (results['Direction']=='EW')]\n",
    "        to_drop = []\n",
    "        for result_index, result in validate_results.iterrows():\n",
    "            event_data = data[(data['ObjectID']==result['ObjectID']) & (data['TimeIndex']>(result['TimeIndex']-5)) & (data['TimeIndex']<(result['TimeIndex']+5))]\n",
    "            #TODO : Should I move the event to the data point?\n",
    "            fits_threshold = (event_data[DELTA_COLUMN].abs()>MANEUVER_THRESHOLD).any()\n",
    "            if not fits_threshold:\n",
    "                to_drop.append(result_index)\n",
    "                \n",
    "        for index in to_drop:\n",
    "            results = results.drop(index)\n",
    "        return results\n",
    "\n",
    "INCLINATION_DIR = \"Inclination Direction\"\n",
    "def apply_ns_ik_maneuver_validator(results, data):\n",
    "        validate_results = results[(results['Node']=='IK') & (results['Direction']=='NS') ]\n",
    "        to_drop = []\n",
    "        for result_index, result in validate_results.iterrows():\n",
    "            event_data = data[(data['ObjectID']==result['ObjectID']) & (data['TimeIndex']>(result['TimeIndex']-5)) & (data['TimeIndex']<(result['TimeIndex']+5))]\n",
    "            #TODO : Should I move the event to the data point?\n",
    "            validated = True\n",
    "            direction_sum = event_data[INCLINATION_DIR].sum()\n",
    "            if (direction_sum>=8) | (direction_sum<=-8):\n",
    "                validated = False\n",
    "            if not validated:\n",
    "                to_drop.append(result_index)\n",
    "                \n",
    "        for index in to_drop:\n",
    "            results = results.drop(index)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ns_id_maneuver_validator(results, data):\n",
    "    mask = (results['Node']=='ID') & (results['Direction']=='NS')\n",
    "    validated_results = results[~mask].copy()\n",
    "    return validated_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NodeDetectionEvaluator` class in the evaluation module allows not only to\n",
    "compute the general score for a given dataset, but get evaluations per object, and\n",
    "even plots that show how the predictions look like in a timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_results = utils.convert_classifier_output(train_data)\n",
    "\n",
    "print(\"ML Results: \"+str(train_results.shape[0]))\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_train, train_results, \n",
    "                                              tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "print(f'Precision for the train set: {precision:.2f}')\n",
    "print(f'Recall for the train set: {recall:.2f}')\n",
    "print(f'F2 for the train set: {f2:.2f}')\n",
    "print(f'RMSE for the train set: {rmse:.2f}')\n",
    "\n",
    "# ss_validator = SSValidator()\n",
    "validated_results = apply_ss_validator(train_results, data)\n",
    "\n",
    "print(\"\")\n",
    "print(\"SS Validated: \"+str(validated_results.shape[0]))\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_train, validated_results, \n",
    "                                              tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "print(f'Precision for the train set: {precision:.2f}')\n",
    "print(f'Recall for the train set: {recall:.2f}')\n",
    "print(f'F2 for the train set: {f2:.2f}')\n",
    "print(f'RMSE for the train set: {rmse:.2f}')\n",
    "\n",
    "# maneuver_validator = ManeuverValidator()\n",
    "validated_results = apply_ew_maneuver_validator(validated_results, data)\n",
    "\n",
    "print(\"\")\n",
    "print(\"EW Validated: \"+str(validated_results.shape[0]))\n",
    "\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_train, validated_results, \n",
    "                                              tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "print(f'Precision for the train set: {precision:.2f}')\n",
    "print(f'Recall for the train set: {recall:.2f}')\n",
    "print(f'F2 for the train set: {f2:.2f}')\n",
    "print(f'RMSE for the train set: {rmse:.2f}')\n",
    "\n",
    "# maneuver_validator = ManeuverValidator()\n",
    "validated_results = apply_ns_ik_maneuver_validator(validated_results, data)\n",
    "\n",
    "print(\"\")\n",
    "print(\"NS-IK Validated: \"+str(validated_results.shape[0]))\n",
    "\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_train, validated_results, \n",
    "                                              tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "print(f'Precision for the train set: {precision:.2f}')\n",
    "print(f'Recall for the train set: {recall:.2f}')\n",
    "print(f'F2 for the train set: {f2:.2f}')\n",
    "print(f'RMSE for the train set: {rmse:.2f}')\n",
    "\n",
    "\n",
    "validated_results = apply_ns_id_maneuver_validator(validated_results, data)\n",
    "print(\"\")\n",
    "print(\"NS-ID Validated: \"+str(validated_results.shape[0]))\n",
    "\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_train, validated_results, \n",
    "                                              tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "print(f'Precision for the train set: {precision:.2f}')\n",
    "print(f'Recall for the train set: {recall:.2f}')\n",
    "print(f'F2 for the train set: {f2:.2f}')\n",
    "print(f'RMSE for the train set: {rmse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evaluation timeline for a random ObjectID from the training set\n",
    "evaluator.plot(np.random.choice(train_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the Object IDs in the training set and call the evaluation\n",
    "# function for each object and aggregate the results\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "total_wn = 0\n",
    "total_wt = 0\n",
    "total_nm = 0\n",
    "for oid in train_data['ObjectID'].unique():\n",
    "    tp, fp, fn, gt_object, p_object, wrong_nodes, wrong_type, not_matched, ew_fp, ns_fp,ns_ik_fp, ns_id_fp = evaluator.evaluate(oid)\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "    total_wn += wrong_nodes\n",
    "    total_wt += wrong_type\n",
    "    total_nm += not_matched\n",
    "\n",
    "print(f'Total true positives: {total_tp}')\n",
    "print(f'Total false positives: {total_fp}')\n",
    "print(f'Total false negatives: {total_fn}')\n",
    "print(f'Total wrong nodes: {total_wn}')\n",
    "print(f'Total wrong types: {total_wt}')\n",
    "print(f'Total not matched: {total_nm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if config.valid_ratio > 0:\n",
    "    valid_results = utils.convert_classifier_output(valid_data)\n",
    "    print(\"ML Results: \"+str(valid_results.shape[0]))\n",
    "    evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, \n",
    "                                                  valid_results,\n",
    "                                                  tolerance=config.tolerance)\n",
    "    precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "    print(f'Precision for the validation set: {precision:.2f}')\n",
    "    print(f'Recall for the validation set: {recall:.2f}')\n",
    "    print(f'F2 for the validation set: {f2:.2f}')\n",
    "    print(f'RMSE for the validation set: {rmse:.2f}')\n",
    "\n",
    "    \n",
    "\n",
    "    # ss_validator = SSValidator()\n",
    "    validated_results = apply_ss_validator(valid_results, data)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"SS Validated: \"+str(validated_results.shape[0]))\n",
    "    evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, \n",
    "                                                  validated_results,\n",
    "                                                  tolerance=config.tolerance)\n",
    "    precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "    print(f'Precision for the validation set: {precision:.2f}')\n",
    "    print(f'Recall for the validation set: {recall:.2f}')\n",
    "    print(f'F2 for the validation set: {f2:.2f}')\n",
    "    print(f'RMSE for the validation set: {rmse:.2f}')\n",
    "\n",
    "    # maneuver_validator = ManeuverValidator()\n",
    "    validated_results = apply_ew_maneuver_validator(validated_results, data)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"EW Validated: \"+ str(validated_results.shape[0]))\n",
    "    evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, \n",
    "                                                  validated_results,\n",
    "                                                  tolerance=config.tolerance)\n",
    "    precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "    print(f'Precision for the validation set: {precision:.2f}')\n",
    "    print(f'Recall for the validation set: {recall:.2f}')\n",
    "    print(f'F2 for the validation set: {f2:.2f}')\n",
    "    print(f'RMSE for the validation set: {rmse:.2f}')\n",
    "\n",
    "    validated_results = apply_ns_ik_maneuver_validator(validated_results, data)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"NS-IK Validated: \"+ str(validated_results.shape[0]))\n",
    "    evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, \n",
    "                                                  validated_results,\n",
    "                                                  tolerance=config.tolerance)\n",
    "    precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "    print(f'Precision for the validation set: {precision:.2f}')\n",
    "    print(f'Recall for the validation set: {recall:.2f}')\n",
    "    print(f'F2 for the validation set: {f2:.2f}')\n",
    "    print(f'RMSE for the validation set: {rmse:.2f}')\n",
    "\n",
    "\n",
    "    validated_results = apply_ns_id_maneuver_validator(validated_results, data)\n",
    "    print(\"\")\n",
    "    print(\"NS-ID Validated: \"+ str(validated_results.shape[0]))\n",
    "    evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, \n",
    "                                                  validated_results,\n",
    "                                                  tolerance=config.tolerance)\n",
    "    precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "    print(f'Precision for the validation set: {precision:.2f}')\n",
    "    print(f'Recall for the validation set: {recall:.2f}')\n",
    "    print(f'F2 for the validation set: {f2:.2f}')\n",
    "    print(f'RMSE for the validation set: {rmse:.2f}')\n",
    "# validated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evaluation timeline for a random ObjectID from the training set\n",
    "evaluator.plot(np.random.choice(valid_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained random forest models (and label encoders) to disk\n",
    "# Create the folder trained_model if it doesn't exist\n",
    "Path('trained_model').mkdir(exist_ok=True)\n",
    "pickle.dump(model_EW, open('trained_model/model_EW_v3.pkl', 'wb'))\n",
    "pickle.dump(model_NS, open('trained_model/model_NS_v3.pkl', 'wb'))\n",
    "pickle.dump(le_EW, open('trained_model/le_EW_v3.pkl', 'wb'))\n",
    "pickle.dump(le_NS, open('trained_model/le_NS_v3.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
