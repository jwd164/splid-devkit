{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This is the main script that will create the predictions on test data and save \n",
    "    a predictions file.\n",
    "\"\"\"\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import mlflow\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ew_Validator\n",
    "import ns_id_Validator\n",
    "import ns_ik_Validator\n",
    "import ss_Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Tabularization Complete\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# INPUT/OUTPUT PATHS WITHIN THE DOCKER CONTAINER\n",
    "TRAINED_MODEL_DIR = Path('./trained_model/')\n",
    "TEST_DATA_DIR = Path('../../dataset/data_subset/test/')\n",
    "TEST_PREDS_FP = Path('../../submission/submission.csv')\n",
    "\n",
    "\n",
    "ss_validator = ss_Validator.SS_Validator()\n",
    "ew_validator = ew_Validator.EW_Validator()\n",
    "ns_ik_validator = ns_ik_Validator.NS_IK_Validator()\n",
    "ns_id_validator = ns_id_Validator.NS_ID_Validator()\n",
    "\n",
    "# Rest of configuration, specific to this submission\n",
    "delta_column = \"Delta_SemimajorAxis\"\n",
    "\n",
    "feature_cols_EW = [\n",
    "    \"Eccentricity\",\n",
    "    \"Semimajor Axis (m)\",\n",
    "    # \"Inclination (deg)\",\n",
    "    \"RAAN (deg)\",\n",
    "    \"Argument of Periapsis (deg)\",\n",
    "    # \"True Anomaly (deg)\",\n",
    "    # \"Latitude (deg)\",\n",
    "    \"Longitude (deg)\",\n",
    "    \"Altitude (m)\",\n",
    "    # \"X (m)\",\n",
    "    # \"Y (m)\",\n",
    "    # \"Z (m)\",\n",
    "    # \"Vx (m/s)\",\n",
    "    # \"Vy (m/s)\",\n",
    "    # \"Vz (m/s)\",\n",
    "    # delta_column,\n",
    "]\n",
    "\n",
    "lag_steps = 0\n",
    "\n",
    "test_data, updated_feature_cols = utils.tabularize_data(\n",
    "    TEST_DATA_DIR, feature_cols, lag_steps=lag_steps)\n",
    "\n",
    "print(\"Data Tabularization Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the trained models (don't use the utils module, use pickle)\n",
    "model_EW = pickle.load(open(TRAINED_MODEL_DIR / 'model_EW.pkl', 'rb'))\n",
    "le_EW = pickle.load(open(TRAINED_MODEL_DIR / 'le_EW.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "# mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "# mlflow.set_experiment(\"ARCLab Competition\")\n",
    "# mlflow.sklearn.autolog(log_models = True)\n",
    "\n",
    "# with mlflow.start_run():\n",
    "# Make predictions on the test data for EW\n",
    "test_data['Predicted_EW'] = le_EW.inverse_transform(\n",
    "    model_EW.predict(test_data[updated_feature_cols])\n",
    ")\n",
    "# Trash the models to free up the memory.\n",
    "model_EW = None\n",
    "le_EW = None\n",
    "\n",
    "print(\"EW Predictions are done\")\n",
    "\n",
    "model_NS = pickle.load(open(TRAINED_MODEL_DIR / 'model_NS.pkl', 'rb'))\n",
    "le_NS = pickle.load(open(TRAINED_MODEL_DIR / 'le_NS.pkl', 'rb'))\n",
    "\n",
    "# Make predictions on the test data for NS\n",
    "test_data['Predicted_NS'] = le_NS.inverse_transform(\n",
    "    model_NS.predict(test_data[updated_feature_cols])\n",
    ")\n",
    "\n",
    "print(\"NS Predictions are done\")\n",
    "\n",
    "# Print the first few rows of the test data with predictions for both EW and NS\n",
    "test_results = utils.convert_classifier_output(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_results = ss_validator.apply_validator(test_results, test_data)\n",
    "\n",
    "validated_results = ew_validator.apply_validator(test_results, test_data)\n",
    "\n",
    "\n",
    "validated_results = ns_ik_validator.apply_validator(test_results, test_data)\n",
    "\n",
    "\n",
    "validated_results = ns_id_validator.apply_validator(test_results, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_test_results = stripped_test_results.copy()\n",
    "merged_test_results = merged_test_results.sort_values(by=['ObjectID', 'TimeIndex']).reset_index(drop=True)\n",
    "merged_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test results to a csv file to be submitted to the challenge\n",
    "merged_test_results.to_csv(TEST_PREDS_FP, index=False)\n",
    "print(\"Saved predictions to: {}\".format(TEST_PREDS_FP))\n",
    "\n",
    "# time.sleep(360) # TEMPORARY FIX TO OVERCOME EVALAI BUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joe\\Desktop\\Projects\\ARCLab-MIT\\splid-devkit\\baseline_submissions\\ml_python\n",
      "Total TPs: 15\n",
      "Total FPs: 319\n",
      "Total FNs: 4\n",
      "Total Distances: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Total Wrong Nodes: 1\n",
      "Total Wrong Types: 2\n",
      "Total Not Matched: 316\n",
      "Total EW FP: 1\n",
      "Total NS FP: 318\n",
      "\n",
      "Precision: 0.04\n",
      "Recall: 0.79\n",
      "F2: 0.18\n",
      "RMSE: 0.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(os.path.abspath(os.curdir))\n",
    "\n",
    "module_paths = [\n",
    "    os.path.abspath(os.path.join('../..')),\n",
    "]\n",
    "for module_path in module_paths:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "from baseline_submissions.evaluation import NodeDetectionEvaluator\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truth_df = pd.read_csv('../../dataset/data_subset/train_labels.csv')\n",
    "test_results = pd.read_csv('../../submission/submission.csv')\n",
    "\n",
    "validated_results = ss_validator.apply_validator(test_results, test_data)\n",
    "validated_results = ew_validator.apply_validator(validated_results, test_data)\n",
    "validated_results = ns_ik_validator.apply_validator(validated_results, test_data)\n",
    "validated_results = ns_id_validator.apply_validator(validated_results, test_data)\n",
    "\n",
    "evaluator = NodeDetectionEvaluator(ground_truth_df, validated_results, tolerance=6)\n",
    "precision, recall, f2, rmse = evaluator.score(debug=True)\n",
    "print(\"\")\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F2: {f2:.2f}')\n",
    "print(f'RMSE: {rmse:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
